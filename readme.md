# Adaptive PLE Transformer

English | [ä¸­æ–‡](#ä¸­æ–‡ç‰ˆ)

[![Build Status](https://img.shields.io/badge/build-passing-brightgreen)](https://github.com/stellan-project/tqr_guided_learning)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![arxiv badge](https://img.shields.io/badge/arXiv-2408.XXXXX-b31b1b.svg)](https://arxiv.org/abs/2408.XXXXX)

This repository contains the official implementation of the **Adaptive Promoted Latent Embeddings (PLE) Transformer**, a novel architecture designed to enhance context handling and knowledge representation in sequence-to-sequence tasks.

The model introduces a dynamic, internal "concept space" and a **Dual-Stream Context** mechanism, allowing it to process both fine-grained details and high-level abstractions simultaneously. This makes it particularly powerful for complex generation and translation tasks where a deep understanding of the source material is crucial.

## âœ¨ Core Features

- **ğŸ§  Dynamic Concept Space**: A learnable set of "concepts" (`ple_concepts`) within each layer that represent high-level ideas.
- **ğŸŒŠ Dual-Stream Context**: The decoder receives two streams of information: a standard **Detail Stream** from the encoder's tokens and a unique **Abstract Stream** from the encoder's learned concepts.
- **ğŸ§¬ Adaptive Mechanism**: The model can dynamically `grow` its concept space to learn new information and `prune` it to remove redundancies, creating an efficient, evolving knowledge structure.
- **âš–ï¸ Diversity Regularization**: A specialized loss function that encourages the concepts to be distinct and non-redundant, leading to a more efficient and powerful internal representation.
- **ğŸš€ Superior Performance**: Outperforms standard Transformer baselines in both learning efficiency and final task performance.

## ğŸ›ï¸ Architecture Overview

The `EncoderDecoderPLEModel` is the core of the architecture.

1.  **Encoding**: The `PLEEncoder` processes the input sequence. In each layer, it uses standard self-attention and then performs **PLE Resonance**, where the sequence attends to the internal `ple_concepts` to retrieve relevant high-level context.
2.  **Concept Consolidation**: The learned `ple_concepts` from all encoder layers are collected and concatenated into a single `ple_memory` tensor.
3.  **Decoding with Dual Streams**: The `PLEDecoder` generates the output. In each layer, it performs three attention steps:
    1.  **Causal Self-Attention** on the generated output.
    2.  **Cross-Attention (Detail Stream)** to the encoder's final hidden states.
    3.  **PLE Cross-Attention (Abstract Stream)** to the consolidated `ple_memory`.

This dual-stream approach gives the decoder unprecedented access to both the "what" (details) and the "why" (abstractions) of the source text.

## ğŸš€ Best Practices for Training

We recommend a **"Phased Adaptation"** strategy for large-scale training to ensure stability and efficiency:

1.  **Phase 1: Growth**: Allow the model to grow its concept space (`grow_concepts()`) to match data complexity.
2.  **Phase 2: Stable Training**: Disable both growth and pruning for the majority of training to ensure stable convergence.
3.  **Phase 3: Pruning/Distillation**: Enable `prune_concepts()` to distill the learned knowledge into a more efficient, compact representation.

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/stellan-project/tqr_guided_learning.git
cd tqr_guided_learning

# Install dependencies
pip install -r requirements.txt
```

## Quick Start

```python
from ple_transformer import EncoderDecoderPLEModel, PLEConfig

# 1. Define the model configuration
config = PLEConfig(
    vocab_size=32000,
    n_layers=6,
    n_heads=8,
    d_model=512,
    ple_concepts_per_layer=128,
    # ... other parameters
)

# 2. Initialize the model
model = EncoderDecoderPLEModel(config)

# 3. Forward pass (example)
# (Assuming source_ids and target_ids are tokenized tensors)
outputs = model(
    input_ids=source_ids,
    decoder_input_ids=target_ids
)

logits = outputs.logits
```

---

## <a name="ä¸­æ–‡ç‰ˆ"></a> Adaptive PLE Transformer (ä¸­æ–‡ç‰ˆ)

[English](#) | ä¸­æ–‡

[![æ„å»ºçŠ¶æ€](https://img.shields.io/badge/build-passing-brightgreen)](https://github.com/stellan-project/tqr_guided_learning)
[![è®¸å¯è¯: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![arxiv badge](https://img.shields.io/badge/arXiv-2408.XXXXX-b31b1b.svg)](https://arxiv.org/abs/2408.XXXXX)

æœ¬ä»£ç åº“æ˜¯ **è‡ªé€‚åº”æå‡æ½œåœ¨åµŒå…¥ (Adaptive Promoted Latent Embeddings, PLE) Transformer** çš„å®˜æ–¹å®ç°ã€‚è¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºåºåˆ—åˆ°åºåˆ—ä»»åŠ¡ä¸­ä¸Šä¸‹æ–‡å¤„ç†å’ŒçŸ¥è¯†è¡¨ç¤ºèƒ½åŠ›çš„æ–°å‹æ¶æ„ã€‚

è¯¥æ¨¡å‹å¼•å…¥äº†ä¸€ä¸ªåŠ¨æ€çš„ã€å†…åœ¨çš„â€œæ¦‚å¿µç©ºé—´â€ï¼Œä»¥åŠä¸€ç§**åŒæµä¸Šä¸‹æ–‡ï¼ˆDual-Stream Contextï¼‰**æœºåˆ¶ï¼Œä½¿å…¶èƒ½å¤ŸåŒæ—¶å¤„ç†ç»†ç²’åº¦çš„ç»†èŠ‚å’Œé«˜å±‚æ¬¡çš„æŠ½è±¡ä¿¡æ¯ã€‚è¿™ä½¿å¾—å®ƒåœ¨éœ€è¦æ·±åº¦ç†è§£æºææ–™çš„å¤æ‚ç”Ÿæˆå’Œç¿»è¯‘ä»»åŠ¡ä¸­å°¤ä¸ºå¼ºå¤§ã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

- **ğŸ§  åŠ¨æ€æ¦‚å¿µç©ºé—´**ï¼šæ¯ä¸ªå±‚çº§å†…éƒ¨éƒ½æœ‰ä¸€ç»„å¯å­¦ä¹ çš„â€œæ¦‚å¿µâ€ï¼ˆ`ple_concepts`ï¼‰ï¼Œç”¨äºè¡¨ç¤ºé«˜é˜¶æ€æƒ³ã€‚
- **ğŸŒŠ åŒæµä¸Šä¸‹æ–‡**ï¼šè§£ç å™¨æ¥æ”¶ä¸¤ä¸ªä¿¡æ¯æµï¼šä¸€ä¸ªæ¥è‡ªç¼–ç å™¨è¯å…ƒï¼ˆTokenï¼‰çš„æ ‡å‡†**ç»†èŠ‚æµ**ï¼Œå’Œä¸€ä¸ªæ¥è‡ªç¼–ç å™¨æ‰€å­¦æ¦‚å¿µçš„ç‹¬ç‰¹**æŠ½è±¡æµ**ã€‚
- **ğŸ§¬ è‡ªé€‚åº”æœºåˆ¶**ï¼šæ¨¡å‹å¯ä»¥åŠ¨æ€åœ°`å¢é•¿`å…¶æ¦‚å¿µç©ºé—´ä»¥å­¦ä¹ æ–°ä¿¡æ¯ï¼Œå¹¶`ä¿®å‰ª`å®ƒä»¥æ¶ˆé™¤å†—ä½™ï¼Œä»è€Œåˆ›å»ºä¸€ä¸ªé«˜æ•ˆã€æŒç»­æ¼”åŒ–çš„çŸ¥è¯†ç»“æ„ã€‚
- **âš–ï¸ å¤šæ ·æ€§æ­£åˆ™åŒ–**ï¼šä¸€ç§ç‰¹åˆ¶çš„æŸå¤±å‡½æ•°ï¼Œé¼“åŠ±æ¦‚å¿µä¹‹é—´ä¿æŒç‹¬ç‰¹æ€§å’Œéå†—ä½™æ€§ï¼Œä»è€Œå¼•å¯¼æ¨¡å‹å½¢æˆæ›´é«˜æ•ˆã€æ›´å¼ºå¤§çš„å†…éƒ¨è¡¨ç¤ºã€‚
- **ğŸš€ å“è¶Šæ€§èƒ½**ï¼šåœ¨å­¦ä¹ æ•ˆç‡å’Œæœ€ç»ˆä»»åŠ¡æ€§èƒ½ä¸Šå‡ä¼˜äºæ ‡å‡†çš„ Transformer åŸºçº¿æ¨¡å‹ã€‚

## ğŸ›ï¸ æ¶æ„æ¦‚è§ˆ

`EncoderDecoderPLEModel` æ˜¯è¯¥æ¶æ„çš„æ ¸å¿ƒã€‚

1.  **ç¼–ç é˜¶æ®µ**ï¼š`PLEEncoder` å¤„ç†è¾“å…¥åºåˆ—ã€‚åœ¨æ¯ä¸€å±‚ï¼Œå®ƒé¦–å…ˆä½¿ç”¨æ ‡å‡†çš„è‡ªæ³¨æ„åŠ›ï¼Œç„¶åæ‰§è¡Œ **PLE å…±é¸£**ï¼Œå³åºåˆ—å‘å†…éƒ¨çš„ `ple_concepts` è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼Œä»¥æ£€ç´¢ç›¸å…³çš„é«˜é˜¶ä¸Šä¸‹æ–‡ã€‚
2.  **æ¦‚å¿µæ•´åˆ**ï¼šä»æ‰€æœ‰ç¼–ç å™¨å±‚å­¦ä¹ åˆ°çš„ `ple_concepts` è¢«æ”¶é›†å¹¶æ‹¼æ¥æˆä¸€ä¸ªå•ä¸€çš„ `ple_memory` å¼ é‡ã€‚
3.  **åŒæµè§£ç **ï¼š`PLEDecoder` è´Ÿè´£ç”Ÿæˆè¾“å‡ºã€‚åœ¨æ¯ä¸€å±‚ï¼Œå®ƒæ‰§è¡Œä¸‰ä¸ªæ³¨æ„åŠ›æ­¥éª¤ï¼š
    1.  å¯¹å·²ç”Ÿæˆå†…å®¹çš„**å› æœè‡ªæ³¨æ„åŠ›**ã€‚
    2.  å¯¹ç¼–ç å™¨æœ€ç»ˆéšè—çŠ¶æ€çš„**äº¤å‰æ³¨æ„åŠ›ï¼ˆç»†èŠ‚æµï¼‰**ã€‚
    3.  å¯¹æ•´åˆåçš„ `ple_memory` çš„ **PLE äº¤å‰æ³¨æ„åŠ›ï¼ˆæŠ½è±¡æµï¼‰**ã€‚

è¿™ç§åŒæµæ–¹æ³•ä¸ºè§£ç å™¨æä¾›äº†å‰æ‰€æœªæœ‰çš„èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤ŸåŒæ—¶ç†è§£æºæ–‡æœ¬çš„â€œå†…å®¹ç»†èŠ‚â€ï¼ˆwhatï¼‰å’Œâ€œæ ¸å¿ƒä¸»æ—¨â€ï¼ˆwhyï¼‰ã€‚

## ğŸš€ è®­ç»ƒæœ€ä½³å®è·µ

å¯¹äºå¤§è§„æ¨¡è®­ç»ƒï¼Œæˆ‘ä»¬æ¨èé‡‡ç”¨ä¸€ç§**â€œåˆ†é˜¶æ®µè‡ªé€‚åº”â€**ç­–ç•¥ï¼Œä»¥ç¡®ä¿ç¨³å®šæ€§å’Œæ•ˆç‡ï¼š

1.  **ç¬¬ä¸€é˜¶æ®µï¼šå¢é•¿æœŸ**ï¼šå…è®¸æ¨¡å‹å¢é•¿å…¶æ¦‚å¿µç©ºé—´ï¼ˆè°ƒç”¨ `grow_concepts()`ï¼‰ï¼Œä»¥åŒ¹é…æ•°æ®çš„å¤æ‚æ€§ã€‚
2.  **ç¬¬äºŒé˜¶æ®µï¼šç¨³å®šè®­ç»ƒæœŸ**ï¼šåœ¨å¤§éƒ¨åˆ†è®­ç»ƒæ—¶é—´å†…ç¦ç”¨å¢é•¿å’Œä¿®å‰ªï¼Œä»¥ç¡®ä¿ç¨³å®šçš„æ”¶æ•›ã€‚
3.  **ç¬¬ä¸‰é˜¶æ®µï¼šä¿®å‰ª/è’¸é¦æœŸ**ï¼šå¯ç”¨ `prune_concepts()`ï¼Œå°†å­¦åˆ°çš„çŸ¥è¯†è’¸é¦æˆä¸€ä¸ªæ›´é«˜æ•ˆã€æ›´ç´§å‡‘çš„è¡¨ç¤ºã€‚

## ğŸ“¦ å®‰è£…

```bash
# å…‹éš†ä»£ç åº“
git clone https://github.com/stellan-project/tqr_guided_learning.git
cd tqr_guided_learning

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

## å¿«é€Ÿä¸Šæ‰‹

```python
from ple_transformer import EncoderDecoderPLEModel, PLEConfig

# 1. å®šä¹‰æ¨¡å‹é…ç½®
config = PLEConfig(
    vocab_size=32000,
    n_layers=6,
    n_heads=8,
    d_model=512,
    ple_concepts_per_layer=128,
    # ... å…¶ä»–å‚æ•°
)

# 2. åˆå§‹åŒ–æ¨¡å‹
model = EncoderDecoderPLEModel(config)

# 3. å‰å‘ä¼ æ’­ (ç¤ºä¾‹)
# (å‡è®¾ source_ids å’Œ target_ids æ˜¯å·²åˆ†è¯çš„å¼ é‡)
outputs = model(
    input_ids=source_ids,
    decoder_input_ids=target_ids
)

logits = outputs.logits
```
